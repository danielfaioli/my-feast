{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb6aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      "com.microsoft.azure#azure-data-lake-store-sdk added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-85b781b0-129e-400a-91e8-75cad92e5bff;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.10 in central\n",
      ":: resolution report :: resolve 2986ms :: artifacts dl 194ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.10 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.8.6 by [com.fasterxml.jackson.core#jackson-core;2.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   26  |   0   |   0   |   2   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-85b781b0-129e-400a-91e8-75cad92e5bff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/97ms)\n",
      "22/05/14 22:47:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from feast import FeatureStore\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.executor.cores\", 1).\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-data-lake-store-sdk:2.3.10\").\\\n",
    "        config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\").\\\n",
    "        config(\"spark.hadoop.fs.azure.account.key.myfeastadls.dfs.core.windows.net\", \"U588DqWLAQQz3zoOkSTij94Me6Wfk+XrmS5Lcd0QePAiGl/LsgkFr76se9scT9w/wagZaEQmppcpTmOZi90DfA==\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "\n",
    "# connect to FS Registry and apply \n",
    "fs = FeatureStore(\"./feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4a7a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"trip_stats_hour.y\",\n",
      "    \"entities\": [\n",
      "      \"VendorID\"\n",
      "    ],\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"avg_trip_dist\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_pass_count\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_tip\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"172800s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"tpep_dropoff_datetime\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"trip_stats_hourly\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"hdfs://namenode:8020/g_features/trip_stats\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-11T02:17:15.556603Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-11T02:17:15.556603Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"trip_tip_hourly\",\n",
      "    \"entities\": [\n",
      "      \"VendorID\"\n",
      "    ],\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"avg_tip_pass\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_tip_trip\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"86400s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"dropoff_hour\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"trip_tip_hourly\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"abfss://gold@myfeastadls.dfs.core.windows.net/tripdata_hourly\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-12T11:24:27.034288Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-12T11:24:27.034288Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for feature in fs.list_feature_views():\n",
    "    print(feature)\n",
    "    print(\"=\"*100+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48dd957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.option(\"header\", True).csv(\"hdfs://namenode:8020/data/yellow_tripdata_2022-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c12287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2022-01-01 00:35:40|  2022-01-01 00:53:29|            2.0|          3.8|       1.0|                 N|         142|         236|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|                 2.5|\n",
      "|       1| 2022-01-01 00:33:43|  2022-01-01 00:42:07|            1.0|          2.1|       1.0|                 N|         236|          42|           1|        8.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        13.3|                 0.0|\n",
      "|       2| 2022-01-01 00:53:21|  2022-01-01 01:02:19|            1.0|         0.97|       1.0|                 N|         166|         166|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|                 0.0|\n",
      "|       2| 2022-01-01 00:25:21|  2022-01-01 00:35:23|            1.0|         1.09|       1.0|                 N|         114|          68|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|\n",
      "|       2| 2022-01-01 00:36:48|  2022-01-01 01:14:20|            1.0|          4.3|       1.0|                 N|          68|         163|           1|       23.5|  0.5|    0.5|       3.0|         0.0|                  0.3|        30.3|                 2.5|\n",
      "|       1| 2022-01-01 00:40:15|  2022-01-01 01:09:48|            1.0|         10.3|       1.0|                 N|         138|         161|           1|       33.0|  3.0|    0.5|      13.0|        6.55|                  0.3|       56.35|                 2.5|\n",
      "|       2| 2022-01-01 00:20:50|  2022-01-01 00:34:58|            1.0|         5.07|       1.0|                 N|         233|          87|           1|       17.0|  0.5|    0.5|       5.2|         0.0|                  0.3|        26.0|                 2.5|\n",
      "|       2| 2022-01-01 00:13:04|  2022-01-01 00:22:45|            1.0|         2.02|       1.0|                 N|         238|         152|           2|        9.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|\n",
      "|       2| 2022-01-01 00:30:02|  2022-01-01 00:44:49|            1.0|         2.71|       1.0|                 N|         166|         236|           1|       12.0|  0.5|    0.5|      2.25|         0.0|                  0.3|       18.05|                 2.5|\n",
      "|       2| 2022-01-01 00:48:52|  2022-01-01 00:53:28|            1.0|         0.78|       1.0|                 N|         236|         141|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.8|                 2.5|\n",
      "|       2| 2022-01-01 00:55:03|  2022-01-01 01:04:25|            1.0|         1.91|       1.0|                 N|         141|         229|           2|        8.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       2| 2022-01-01 00:31:06|  2022-01-01 00:34:14|            3.0|         0.82|       1.0|                 N|         114|          90|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2022-01-01 00:41:07|  2022-01-01 00:44:46|            3.0|         0.73|       1.0|                 N|         234|         113|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2022-01-01 00:56:34|  2022-01-01 01:12:04|            2.0|         2.16|       1.0|                 N|         246|          79|           1|       11.5|  0.5|    0.5|      3.06|         0.0|                  0.3|       18.36|                 2.5|\n",
      "|       2| 2022-01-01 00:39:46|  2022-01-01 00:47:36|            4.0|         1.43|       1.0|                 N|          43|         140|           1|        7.5|  0.5|    0.5|      2.26|         0.0|                  0.3|       13.56|                 2.5|\n",
      "|       2| 2022-01-01 00:58:06|  2022-01-01 01:05:45|            1.0|         1.58|       1.0|                 N|         239|         151|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|\n",
      "|       1| 2022-01-01 00:33:52|  2022-01-01 00:47:28|            3.0|          4.2|       1.0|                 N|         148|         141|           1|       14.0|  2.5|    0.5|      3.45|         0.0|                  0.3|       20.75|                 2.5|\n",
      "|       1| 2022-01-01 00:53:54|  2022-01-01 01:05:20|            2.0|          2.2|       1.0|                 N|         237|         107|           1|        9.5|  2.5|    0.5|      2.55|         0.0|                  0.3|       15.35|                 2.5|\n",
      "|       1| 2022-01-01 00:00:44|  2022-01-01 00:05:29|            1.0|          0.2|       1.0|                 N|           7|           7|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 0.0|\n",
      "|       1| 2022-01-01 00:35:50|  2022-01-01 00:48:33|            2.0|          3.9|       1.0|                 N|         107|         263|           1|       13.0|  3.0|    0.5|      3.35|         0.0|                  0.3|       20.15|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd57cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf_train = sdf.select(\n",
    "    \"VendorID\", \n",
    "    \"tpep_dropoff_datetime\", \n",
    "    \"tip_amount\"\n",
    ").\\\n",
    "withColumn(\"tpep_dropoff_datetime\", F.date_format(\n",
    "    F.to_timestamp(\n",
    "        F.col(\"tpep_dropoff_datetime\")\n",
    "    ), \"dd-MM-yyyy HH:00:00\"\n",
    ")).\\\n",
    "filter(F.col(\"tip_amount\") != 0).\\\n",
    "groupBy(\"VendorID\", \"tpep_dropoff_datetime\").\\\n",
    "agg(F.avg(\"tip_amount\")).\\\n",
    "withColumnRenamed(\"tpep_dropoff_datetime\", \"event_timestamp\").\\\n",
    "withColumn(\"created\", F.to_date(F.col(\"event_timestamp\"), \"dd-MM-yyyy HH:00:00\").cast(\"string\")).\\\n",
    "withColumnRenamed(\"avg(tip_amount)\", \"tip_target\")\n",
    "\n",
    "# sdf_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51aa2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------------+----------+\n",
      "|VendorID|    event_timestamp|        tip_target|   created|\n",
      "+--------+-------------------+------------------+----------+\n",
      "|       1|01-01-2022 06:00:00| 4.683829787234042|2022-01-01|\n",
      "|       2|01-01-2022 14:00:00|3.8104414715719055|2022-01-01|\n",
      "|       1|01-01-2022 15:00:00|3.6820494186046506|2022-01-01|\n",
      "|       1|01-01-2022 14:00:00| 3.679063545150503|2022-01-01|\n",
      "|       1|01-01-2022 23:00:00| 3.672247191011234|2022-01-01|\n",
      "|       2|01-01-2022 16:00:00| 3.918183962264155|2022-01-01|\n",
      "|       2|01-01-2022 17:00:00|3.6507995495495553|2022-01-01|\n",
      "|       2|01-01-2022 22:00:00|3.9246918056562827|2022-01-01|\n",
      "|       1|01-01-2022 05:00:00|3.9317647058823533|2022-01-01|\n",
      "|       1|01-01-2022 13:00:00|3.3963953488372063|2022-01-01|\n",
      "|       1|01-01-2022 17:00:00|3.6229896907216492|2022-01-01|\n",
      "|       1|01-01-2022 16:00:00|3.7996709585121597|2022-01-01|\n",
      "|       2|01-01-2022 00:00:00| 3.264769514443767|2022-01-01|\n",
      "|       2|01-01-2022 15:00:00|3.9367729831144467|2022-01-01|\n",
      "|       2|01-01-2022 09:00:00| 4.194423480083852|2022-01-01|\n",
      "|       1|01-01-2022 12:00:00|3.4302247191011257|2022-01-01|\n",
      "|       2|01-01-2022 08:00:00|4.6206916426512965|2022-01-01|\n",
      "|       2|01-01-2022 01:00:00| 3.546145870575621|2022-01-01|\n",
      "|       2|01-01-2022 04:00:00|3.7646990740740724|2022-01-01|\n",
      "|       2|01-01-2022 05:00:00| 4.336563307493539|2022-01-01|\n",
      "+--------+-------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_train.filter(F.col(\"created\") == \"2022-01-01\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97207126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- event_timestamp: string (nullable = true)\n",
      " |-- tip_target: double (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1923d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2022 10:55:51 PM WARNING:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/typedef/typehints.py:160: DeprecationWarning: Converting `np.character` to a dtype is deprecated. The current result is `np.dtype(np.str_)` which is not strictly correct. Note that `np.character` is generally deprecated and 'S1' should be used.\n",
      "  elif tpe in (bytes, np.character, np.bytes_, np.string_):\n"
     ]
    }
   ],
   "source": [
    "psdf = sdf_train.filter(F.col(\"event_timestamp\") == \"01-01-2022 06:00:00\").to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bad22d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 22:55:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/14 22:55:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/14 22:55:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/14 22:55:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>tip_target</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01-01-2022 06:00:00</td>\n",
       "      <td>4.68383</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>01-01-2022 06:00:00</td>\n",
       "      <td>4.61000</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VendorID      event_timestamp  tip_target     created\n",
       "0        1  01-01-2022 06:00:00     4.68383  2022-01-01\n",
       "1        2  01-01-2022 06:00:00     4.61000  2022-01-01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe23a435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>tip_target</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01-01-2022 06:00:00</td>\n",
       "      <td>4.68383</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>01-01-2022 06:00:00</td>\n",
       "      <td>4.61000</td>\n",
       "      <td>2022-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VendorID      event_timestamp  tip_target     created\n",
       "0        1  01-01-2022 06:00:00     4.68383  2022-01-01\n",
       "1        2  01-01-2022 06:00:00     4.61000  2022-01-01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = psdf.to_pandas()\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb66a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to FS Registry and apply \n",
    "fs = FeatureStore(\"./feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc84e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from feast import FeatureStore\n",
    "from feast.repo_config import RepoConfig\n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "\n",
    "entity_df = pd.DataFrame(\n",
    "    {\n",
    "        \"event_timestamp\": [\n",
    "            pd.Timestamp(dt, unit=\"ms\", tz=\"UTC\").round(\"ms\")\n",
    "            for dt in pd.date_range(\n",
    "                start=datetime.now() - timedelta(days=180),\n",
    "                end=datetime.now(),\n",
    "                periods=3,\n",
    "            )\n",
    "        ],\n",
    "        \"VendorID\": [1, 2, 3],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874fd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast_azure_provider.registry_store import AzBlobRegistryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d36e8b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>VendorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-15 22:59:11.801000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-13 22:59:11.801000+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-14 22:59:11.801000+00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   event_timestamp  VendorID\n",
       "0 2021-11-15 22:59:11.801000+00:00         1\n",
       "1 2022-02-13 22:59:11.801000+00:00         2\n",
       "2 2022-05-14 22:59:11.801000+00:00         3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b606897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "fs_conf = FeatureStore(\"./cloud_feature_repo/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bbbecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:119: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = fs.get_historical_features(\n",
    "    entity_df=pdf,\n",
    "    features=[\n",
    "        \"trip_tip_hourly:avg_tip_pass\",\n",
    "        \"trip_tip_hourly:avg_tip_trip\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6771e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+----------+------------+------------+\n",
      "|VendorID|    event_timestamp|       tip_target|   created|avg_tip_pass|avg_tip_trip|\n",
      "+--------+-------------------+-----------------+----------+------------+------------+\n",
      "|       2|2022-01-01 06:00:00|4.609999999999999|2022-01-01|        null|        null|\n",
      "|       1|2022-01-01 06:00:00|4.683829787234042|2022-01-01|        null|        null|\n",
      "+--------+-------------------+-----------------+----------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------                                        \n",
      "Exception occurred during processing of request from ('127.0.0.1', 36090)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data.to_spark_df().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef67620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myfeast",
   "language": "python",
   "name": "myfeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
