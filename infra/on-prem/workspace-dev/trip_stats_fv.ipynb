{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8082e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b020f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/14 21:53:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.executor.cores\", 1).\\\n",
    "        getOrCreate()\n",
    "\n",
    "# config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-data-lake-store-sdk:2.3.10\").\\\n",
    "#         config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\").\\\n",
    "#         config(\"spark.hadoop.fs.azure.account.key.myfeastadls.dfs.core.windows.net\", \"U588DqWLAQQz3zoOkSTij94Me6Wfk+XrmS5Lcd0QePAiGl/LsgkFr76se9scT9w/wagZaEQmppcpTmOZi90DfA==\").\\\n",
    "#         config(\"spark.driver.extraClassPath\", \"/usr/bin/spark-3.2.1-bin-hadoop2.7/jars/hadoop-azure-3.3.1.jar,/usr/bin/spark-3.2.1-bin-hadoop2.7/jars/azure-data-lake-store-sdk-2.3.10.jar\").\\\n",
    "#         config(\"spark.executor.extraClassPath\", \"/usr/bin/spark-3.2.1-bin-hadoop2.7/jars/hadoop-azure-3.3.1.jar,/usr/bin/spark-3.2.1-bin-hadoop2.7/jars/azure-data-lake-store-sdk-2.3.10.jar\").\\\n",
    "\n",
    "hdfs = \"hdfs://namenode:8020\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7fc1e",
   "metadata": {},
   "source": [
    "# READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc5b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bd8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set(\"fs.azure.account.key.myfeastadls.dfs.core.windows.net\", \"U588DqWLAQQz3zoOkSTij94Me6Wfk+XrmS5Lcd0QePAiGl/LsgkFr76se9scT9w/wagZaEQmppcpTmOZi90DfA==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ad31dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/14 21:53:31 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: abfss://gold@myfeastadls.dfs.core.windows.net/tripdata_hourly.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabfss://gold@myfeastadls.dfs.core.windows.net/tripdata_hourly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:747)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:745)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:577)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(\"abfss://gold@myfeastadls.dfs.core.windows.net/tripdata_hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d5d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_data = spark.read.option(\"header\", True).csv(f\"{hdfs}/data/yellow_tripdata_2022-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6274c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa8a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2022-01-01 00:35:40|  2022-01-01 00:53:29|            2.0|          3.8|       1.0|                 N|         142|         236|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|                 2.5|\n",
      "|       1| 2022-01-01 00:33:43|  2022-01-01 00:42:07|            1.0|          2.1|       1.0|                 N|         236|          42|           1|        8.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        13.3|                 0.0|\n",
      "|       2| 2022-01-01 00:53:21|  2022-01-01 01:02:19|            1.0|         0.97|       1.0|                 N|         166|         166|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|                 0.0|\n",
      "|       2| 2022-01-01 00:25:21|  2022-01-01 00:35:23|            1.0|         1.09|       1.0|                 N|         114|          68|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|\n",
      "|       2| 2022-01-01 00:36:48|  2022-01-01 01:14:20|            1.0|          4.3|       1.0|                 N|          68|         163|           1|       23.5|  0.5|    0.5|       3.0|         0.0|                  0.3|        30.3|                 2.5|\n",
      "|       1| 2022-01-01 00:40:15|  2022-01-01 01:09:48|            1.0|         10.3|       1.0|                 N|         138|         161|           1|       33.0|  3.0|    0.5|      13.0|        6.55|                  0.3|       56.35|                 2.5|\n",
      "|       2| 2022-01-01 00:20:50|  2022-01-01 00:34:58|            1.0|         5.07|       1.0|                 N|         233|          87|           1|       17.0|  0.5|    0.5|       5.2|         0.0|                  0.3|        26.0|                 2.5|\n",
      "|       2| 2022-01-01 00:13:04|  2022-01-01 00:22:45|            1.0|         2.02|       1.0|                 N|         238|         152|           2|        9.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|\n",
      "|       2| 2022-01-01 00:30:02|  2022-01-01 00:44:49|            1.0|         2.71|       1.0|                 N|         166|         236|           1|       12.0|  0.5|    0.5|      2.25|         0.0|                  0.3|       18.05|                 2.5|\n",
      "|       2| 2022-01-01 00:48:52|  2022-01-01 00:53:28|            1.0|         0.78|       1.0|                 N|         236|         141|           2|        5.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.8|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de46806",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8bfeded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "trip_data = trip_data.select(\"VendorID\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"tip_amount\")\n",
    "\n",
    "trip_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e03c28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = trip_data.\\\n",
    "withColumn(\"created\", F.to_date(F.col(\"tpep_dropoff_datetime\"))).\\\n",
    "withColumn(\"tpep_dropoff_datetime\", F.date_format(\n",
    "    F.to_timestamp(\n",
    "        F.col(\"tpep_dropoff_datetime\")\n",
    "    ), \"dd-MM-yyyy HH:00:00\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95356412",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_stats = trip_data.\\\n",
    "groupby(\"VendorID\", \"tpep_dropoff_datetime\").\\\n",
    "agg(\n",
    "    F.avg(F.col(\"passenger_count\")),\n",
    "    F.avg(F.col(\"trip_distance\")),\n",
    "    F.avg(F.col(\"tip_amount\")),\n",
    "    F.count(F.col(\"VendorID\")),\n",
    "    F.min(F.col(\"trip_distance\")),\n",
    "    F.max(F.col(\"trip_distance\"))\n",
    ").\\\n",
    "withColumnRenamed(\"avg(passenger_count)\", \"avg_pass_count\").\\\n",
    "withColumnRenamed(\"avg(trip_distance)\", \"avg_trip_dist\").\\\n",
    "withColumnRenamed(\"avg(tip_amount)\", \"avg_tip\").\\\n",
    "withColumnRenamed(\"count(VendorID)\", \"num_trip\").\\\n",
    "withColumnRenamed(\"min(trip_distance)\", \"min_trip_dist\").\\\n",
    "withColumnRenamed(\"max(trip_distance)\", \"max_trip_dist\").\\\n",
    "withColumn(\"created\", F.to_date(F.col(\"tpep_dropoff_datetime\"), \"dd-MM-yyyy HH:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cece31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+------------------+------------------+------------------+--------+-------------+-------------+----------+\n",
      "|VendorID|tpep_dropoff_datetime|    avg_pass_count|     avg_trip_dist|           avg_tip|num_trip|min_trip_dist|max_trip_dist|   created|\n",
      "+--------+---------------------+------------------+------------------+------------------+--------+-------------+-------------+----------+\n",
      "|       1|  01-01-2022 00:00:00|1.4976580796252927| 2.637687861271676|2.1924046242774566|     865|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 01:00:00|1.4939673527324344| 3.248910751932531| 2.607554462403373|    1423|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 02:00:00|1.3895985401459854|3.0356820234868995|2.4877326106594446|    1107|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 03:00:00|1.4297297297297298| 3.200942126514135|2.2994751009421255|     743|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 04:00:00|1.3603082851637764|3.7814531548757144|2.5489483747609936|     523|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 05:00:00| 1.295275590551181| 4.551764705882354|2.3630196078431376|     255|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 06:00:00| 1.177914110429448| 4.892727272727273|2.6689090909090902|     165|          0.0|          8.2|2022-01-01|\n",
      "|       1|  01-01-2022 07:00:00|1.1666666666666667|           4.60375|          2.431875|     240|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 08:00:00|1.1524663677130045| 4.468281938325989|2.2436563876651983|     227|          0.0|          9.7|2022-01-01|\n",
      "|       1|  01-01-2022 09:00:00|1.1964912280701754| 4.017182130584194|  1.95810996563574|     291|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 10:00:00|1.3293172690763053|3.9736111111111123| 2.110376984126984|     504|          0.0|          9.7|2022-01-01|\n",
      "|       1|  01-01-2022 11:00:00|1.4534351145038167|3.5335832083958008| 2.206206896551724|     667|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 12:00:00|1.4913366336633664| 3.476528117359414| 2.243997555012226|     818|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 13:00:00|1.4257854821235103| 3.926624068157615| 2.182705005324812|     939|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 14:00:00| 1.439153439153439| 3.923670490093848|2.2991553701772682|     959|          0.0|         98.9|2022-01-01|\n",
      "|       1|  01-01-2022 15:00:00|1.5167420814479637|3.9505357142857114|2.2668660714285713|    1120|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 16:00:00|1.5018726591760299|4.0565777368905245|2.4498988040478378|    1087|          0.0|         96.1|2022-01-01|\n",
      "|       1|  01-01-2022 17:00:00|1.5176125244618395|3.8076254826254883|2.3788416988416983|    1036|          0.0|          9.9|2022-01-01|\n",
      "|       1|  01-01-2022 18:00:00|1.4449950445986124|3.5668302257114823| 2.439882237487733|    1019|          0.0|          9.8|2022-01-01|\n",
      "|       1|  01-01-2022 19:00:00|1.4259448416751788|3.6085341365461874|2.5609538152610423|     996|          0.0|          9.9|2022-01-01|\n",
      "+--------+---------------------+------------------+------------------+------------------+--------+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88dd303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_stats.write.mode(\"overwrite\").partitionBy(\"created\").save(f\"{hdfs}/g_features/trip_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb4b82",
   "metadata": {},
   "source": [
    "# REGISTER FEATURES TO FEATURE STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08a685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/workspace/venv/lib/python3.9/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from feast.infra.offline_stores.contrib.spark_offline_store.spark_source import SparkSource\n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "\n",
    "hdfs = \"hdfs://namenode:8020\"\n",
    "\n",
    "# Feature Source Definition\n",
    "trip_stats_source = SparkSource(\n",
    "    file_format=\"parquet\",\n",
    "    path=f\"{hdfs}/g_features/trip_stats\",\n",
    "    timestamp_field=\"tpep_dropoff_datetime\",\n",
    "    created_timestamp_column=\"created\",\n",
    "    name=\"trip_stats_hourly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf81e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/workspace/venv/lib/python3.9/site-packages/feast/feature_view.py:256: DeprecationWarning: batch_source and stream_source have been deprecated in favor of `source`.The deprecated fields will be removed in Feast 0.23.\n",
      "  warnings.warn(\n",
      "/opt/workspace/venv/lib/python3.9/site-packages/feast/feature_view.py:194: DeprecationWarning: The `features` parameter is being deprecated in favor of the `schema` parameter. Please switch from using `features` to `schema`. This will also requiring switching feature definitions from using `Feature` to `Field`. Feast 0.21 and onwards will not support the `features` parameter.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from feast import Feature, FeatureView, ValueType\n",
    "from datetime import timedelta\n",
    "from feast import Entity\n",
    "\n",
    "# Feature Definition\n",
    "trip_stats_fv = FeatureView(\n",
    "    name=\"trip_stats_hour.y\",\n",
    "    entities=[\"VendorID\"],\n",
    "    features=[\n",
    "        Feature(name=\"avg_trip_dist\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"avg_pass_count\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"avg_tip\", dtype=ValueType.FLOAT),\n",
    "    ],\n",
    "    batch_source=trip_stats_source,\n",
    "    ttl=timedelta(days=2)\n",
    ")\n",
    "\n",
    "# Entity definition => entity == primary key \n",
    "\n",
    "vendor = Entity(name=\"VendorID\", value_type=ValueType.INT64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e511009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "# connect to FS Registry and apply \n",
    "fs = FeatureStore(\"/opt/workspace/feature_repo\")\n",
    "fs.apply([vendor, trip_stats_fv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "789dcf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource at 0x7f924143de20>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_sources = fs.list_data_sources()\n",
    "\n",
    "fs_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3539cc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/workspace/venv/lib/python3.9/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_views = fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb8f3484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FeatureView(name = trip_stats_hour.y, entities = ['VendorID'], stream_source = None, batch_source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"tpep_dropoff_datetime\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"trip_stats_hourly\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"hdfs://namenode:8020/g_features/trip_stats\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"tpep_dropoff_datetime\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"trip_stats_hourly\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"hdfs://namenode:8020/g_features/trip_stats\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, ttl = 2 days, 0:00:00, schema = [avg_trip_dist-Float32, avg_pass_count-Float32, avg_tip-Float32], features = [avg_trip_dist-Float32, avg_pass_count-Float32, avg_tip-Float32], description = , tags = {}, owner = , projection = FeatureViewProjection(name='trip_stats_hour.y', name_alias=None, features=[avg_trip_dist-Float32, avg_pass_count-Float32, avg_tip-Float32], join_key_map={}), created_timestamp = 2022-05-11 02:17:15.556603, last_updated_timestamp = 2022-05-11 02:17:15.556603, online = True, materialization_intervals = [])>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eec0526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a31107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b273d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cookiecutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d3b70a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeastConfigError",
     "evalue": "1 validation error for RepoConfig\n__root__\n   (type=assertion_error)\nat /opt/workspace/feature_repo/feature_store.yaml",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/repo_config.py:352\u001b[0m, in \u001b[0;36mload_repo_config\u001b[0;34m(repo_path)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mRepoConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     c\u001b[38;5;241m.\u001b[39mrepo_path \u001b[38;5;241m=\u001b[39m repo_path\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/repo_config.py:117\u001b[0m, in \u001b[0;36mRepoConfig.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata: Any):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_store, Dict):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pydantic/main.py:331\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RepoConfig\n__root__\n   (type=assertion_error)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFeastConfigError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureStore\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# connect to FS Registry and apply \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureStore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/workspace/feature_repo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/usage.py:338\u001b[0m, in \u001b[0;36mlog_exceptions.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m _produce_event(ctx)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m traceback:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/usage.py:324\u001b[0m, in \u001b[0;36mlog_exceptions.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m fn_call \u001b[38;5;241m=\u001b[39m FnCall(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex, fn_name\u001b[38;5;241m=\u001b[39m_fn_fullname(func), start\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[1;32m    322\u001b[0m )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     _, exc, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/feature_store.py:122\u001b[0m, in \u001b[0;36mFeatureStore.__init__\u001b[0;34m(self, repo_path, config)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m repo_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_path \u001b[38;5;241m=\u001b[39m Path(repo_path)\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mload_repo_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify one of repo_path or config.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/feast/repo_config.py:356\u001b[0m, in \u001b[0;36mload_repo_config\u001b[0;34m(repo_path)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeastConfigError(e, config_path)\n",
      "\u001b[0;31mFeastConfigError\u001b[0m: 1 validation error for RepoConfig\n__root__\n   (type=assertion_error)\nat /opt/workspace/feature_repo/feature_store.yaml"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "# os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "# os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "# os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "# connect to FS Registry and apply \n",
    "fs = FeatureStore(\"/opt/workspace/feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34623500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast SDK Version: \"feast 0.21.0\"\n"
     ]
    }
   ],
   "source": [
    "!feast version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd23d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531e74f131be1abd4e3aae243b5023f7ab8d4006314c7caab0b49f1b9daf0438"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
