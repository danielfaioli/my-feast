{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ed325e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.executor.cores\", 1).\\\n",
    "        getOrCreate()\n",
    "#dynamicaly overwrite partitions \n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "hdfs = \"hdfs://namenode:8020\"\n",
    "\n",
    "def bronze_read(bpath: str) -> DataFrame:\n",
    "    return spark.read.load(bpath)\n",
    "\n",
    "def s_transformation(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # get list of all fields in each message\n",
    "    keys = df.\\\n",
    "        select(F.explode(F.col(\"Body\"))).\\\n",
    "        select(\"key\").\\\n",
    "        distinct().\\\n",
    "        rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # explode dataframe to multiple columns\n",
    "    exprs = [F.col(\"Body\").getItem(k).alias(k) for k in keys]\n",
    "    sdf = df.select(*exprs)\n",
    "    \n",
    "    s_cols = [\n",
    "        \"trip_id\",\n",
    "        \"taxi_id\",\n",
    "        \"trip_start_timestamp\",\n",
    "        \"trip_seconds\",\n",
    "        \"trip_miles\",\n",
    "        \"fare\",\n",
    "        \"tips\",\n",
    "        \"tolls\",\n",
    "        \"extras\",\n",
    "        \"trip_total\"\n",
    "    ]\n",
    "    \n",
    "    # generate silver datafreme\n",
    "    \n",
    "    sdf = sdf.\\\n",
    "        select(*s_cols).\\\n",
    "        withColumn(\"created\", F.to_date(\"trip_start_timestamp\"))\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "def s_writer(sdf: DataFrame, spath, partitionBy = None, mode = \"overwrite\") -> None:\n",
    "    sdf.\\\n",
    "        repartition(\"created\").\\\n",
    "        write.\\\n",
    "        mode(\"overwrite\").\\\n",
    "        partitionBy(\"created\").\\\n",
    "        save(\"hdfs://namenode:8020/silver/chicago/taxi_trips\")\n",
    "    \n",
    "df = spark.read.load(f\"{hdfs}/bronze/chicago/taxi\")\n",
    "\n",
    "sdf = s_transformation(df)\n",
    "\n",
    "s_writer(\n",
    "    sdf=sdf,\n",
    "    spath=f\"{hdfs}/silver/chicago/taxi_trips\",\n",
    "    partitionBy=\"created\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myfeast",
   "language": "python",
   "name": "myfeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
