{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91298cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.account.key.myfeastadls.dfs.core.windows.net\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      "com.microsoft.azure#azure-data-lake-store-sdk added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-40481444-7de3-46fc-a532-1ecf3063df0a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.10 in central\n",
      ":: resolution report :: resolve 750ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.10 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.8.6 by [com.fasterxml.jackson.core#jackson-core;2.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   26  |   0   |   0   |   2   ||   24  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-40481444-7de3-46fc-a532-1ecf3063df0a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 24 already retrieved (0kB/17ms)\n",
      "22/06/29 02:42:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from feast import FeatureStore\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.executor.cores\", 1).\\\n",
    "        config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-data-lake-store-sdk:2.3.10\").\\\n",
    "        config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\").\\\n",
    "        config(\"fs.azure.account.key.myfeastadls.dfs.core.windows.net\", os.environ[\"STORAGE_ACCOUNT_KEY\"]).\\\n",
    "        config(\"spark.databricks.delta.formatCheck.enabled\", False).\\\n",
    "        getOrCreate()\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.myfeastadls.dfs.core.windows.net\", os.environ[\"STORAGE_ACCOUNT_KEY\"])\n",
    "\n",
    "hdfs = \"hdfs://namenode:8020\"\n",
    "fs = FeatureStore(\"./fs_online\")\n",
    "\n",
    "# Another option - authenticate with oauth2\n",
    "    # https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/#rdd-api-1\n",
    "    # https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#--spark-configuration\n",
    "\n",
    "    # spark.hadoop.fs.azure.account.oauth2.client.id.<datalake>.dfs.core.windows.net <sp client id>\n",
    "    # spark.hadoop.fs.azure.account.auth.type.<datalake>.dfs.core.windows.net OAuth\n",
    "    # spark.hadoop.fs.azure.account.oauth.provider.type.<datalake>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n",
    "    # org.apache.hadoop.fs.azure.account.oauth2.client.secret.<datalake>.dfs.core.windows.net {{secrets/secret/secret}}\n",
    "    # spark.hadoop.fs.azure.account.oauth2.client.endpoint.<datalake>.dfs.core.windows.net https://login.microsoftonline.com/<tenant>/oauth2/token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea60c7",
   "metadata": {},
   "source": [
    "# Feature Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f999e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities\n",
      "****************************************************************************************************\n",
      "\n",
      "Entity: driver\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"driver\",\n",
      "    \"valueType\": \"STRING\",\n",
      "    \"joinKey\": \"taxi_id\"\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-19T19:43:17.269765Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-19T19:43:17.269765Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "Entity: trip_id\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"trip_id\",\n",
      "    \"valueType\": \"STRING\",\n",
      "    \"joinKey\": \"trip_id\"\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-23T01:05:37.720234Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-23T01:05:37.720234Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "Entity: taxi_id\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"taxi_id\",\n",
      "    \"valueType\": \"STRING\",\n",
      "    \"joinKey\": \"taxi_id\"\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-23T01:15:19.459273Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-23T01:15:19.459273Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "Entity: read_id\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"read_id\",\n",
      "    \"valueType\": \"STRING\",\n",
      "    \"joinKey\": \"read_id\"\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-06-28T10:49:59.739764Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-06-28T10:49:59.739764Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "\n",
      "Feature Views\n",
      "****************************************************************************************************\n",
      "\n",
      "Feature View: fv_chi_driver_stats_hourly\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"fv_chi_driver_stats_hourly\",\n",
      "    \"entities\": [\n",
      "      \"driver\"\n",
      "    ],\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"avg_trip_time\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_dist\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_fare\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_tips\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"total_tips_hour\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"trips_count\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"0s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"event_timestamp\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"f_chi_driver_stats_hourly\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"hdfs://namenode:8020/gold/chicago/f_driver_stats_hourly\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-19T19:43:17.269395Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-19T19:43:17.269395Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "Feature View: fv_chi_taxi_trips_hourly\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"fv_chi_taxi_trips_hourly\",\n",
      "    \"entities\": [\n",
      "      \"taxi_id\"\n",
      "    ],\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"avg_trip_time\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_dist\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_fare\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_trip_tips\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"total_tips_hour\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"trips_count\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"0s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"event_timestamp\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"chi_taxi_trips_hourly\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"hdfs://namenode:8020/gold/chicago/f_taxi_trips_hourly\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-23T01:15:19.458642Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-23T01:57:11.981748Z\",\n",
      "    \"materializationIntervals\": [\n",
      "      {\n",
      "        \"startTime\": \"2022-04-01T00:00:00Z\",\n",
      "        \"endTime\": \"2022-05-30T00:00:00Z\"\n",
      "      },\n",
      "      {\n",
      "        \"startTime\": \"2022-04-01T00:00:00Z\",\n",
      "        \"endTime\": \"2022-05-30T00:00:00Z\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n",
      "Feature View: fv_chi_station_reads_hourly\n",
      "\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"fv_chi_station_reads_hourly\",\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"precipitation_type\",\n",
      "        \"valueType\": \"STRING\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_temp\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"total_rain\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"0s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"event_timestamp\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"chi_station_reads_hourly_fv\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"abfss://gold@myfeastadls.dfs.core.windows.net/chicago/weather/station_reads_hourly_fv\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-06-28T23:13:05.673147Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-06-28T23:13:05.673147Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature Discovery\n",
    "\n",
    "fs = FeatureStore(\"./fs_online\")\n",
    "\n",
    "entities = fs.list_entities()\n",
    "print(f\"Entities\\n{'*'*100}\\n\")\n",
    "for en in entities:\n",
    "    print(f\"Entity: {en.name}\\n\")\n",
    "    print(en)\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    \n",
    "print(f\"\\nFeature Views\\n{'*'*100}\\n\")\n",
    "for f in fs.list_feature_views(): \n",
    "    print(f\"Feature View: {f.name}\\n\")\n",
    "    print(f)\n",
    "    print(\"=\"*100+\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ddbefa",
   "metadata": {},
   "source": [
    "# Get Historical Data from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b96e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "entity_df = spark.read.\\\n",
    "load(\"hdfs://namenode:8020/gold/chicago/f_taxi_trips_hourly\").filter(F.col(\"created\") <= \"2022-04-15\").select(\"taxi_id\", \"event_timestamp\")\n",
    "# withColumn(\"read_id\", (F.unix_timestamp(F.col(\"event_timestamp\"),\"dd-MM-yyyy HH:00:00\").cast(\"string\"))).\\\n",
    "# select(\"read_id\", \"event_timestamp\").\\\n",
    "# distinct().\\\n",
    "# sort(\"read_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fbcb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|             taxi_id|    event_timestamp|\n",
      "+--------------------+-------------------+\n",
      "|0c16d63294bfa9a1d...|2022-04-01 13:00:00|\n",
      "|dfe93dd8fbdee8c0f...|2022-04-01 13:00:00|\n",
      "|3c07027096c12ad3f...|2022-04-01 13:00:00|\n",
      "|48c0b5669ed50a0dc...|2022-04-01 13:00:00|\n",
      "|21ccb8006a50da9b1...|2022-04-01 13:00:00|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "entity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b55bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:119: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_features = fs.get_historical_features(\n",
    "    entity_df=entity_df.toPandas(),\n",
    "    features=[\n",
    "        \"fv_chi_station_reads_hourly:precipitation_type\",\n",
    "        \"fv_chi_station_reads_hourly:avg_temp\",\n",
    "        \"fv_chi_station_reads_hourly:total_rain\",\n",
    "        \"fv_chi_taxi_trips_hourly:avg_trip_time\",\n",
    "        \"fv_chi_taxi_trips_hourly:avg_trip_dist\",\n",
    "        \"fv_chi_taxi_trips_hourly:avg_trip_fare\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62ef5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = hist_features.to_spark_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6ee907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- taxi_id: string (nullable = true)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- precipitation_type: string (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- total_rain: double (nullable = true)\n",
      " |-- avg_trip_time: double (nullable = true)\n",
      " |-- avg_trip_dist: double (nullable = true)\n",
      " |-- avg_trip_fare: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42f688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 02:46:25 WARN TaskSetManager: Stage 6 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:49:17 WARN TaskSetManager: Stage 13 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:49:18 WARN TaskSetManager: Stage 14 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+------------------+----------+------------------+------------------+------------------+\n",
      "|             taxi_id|    event_timestamp|precipitation_type|          avg_temp|total_rain|     avg_trip_time|     avg_trip_dist|     avg_trip_fare|\n",
      "+--------------------+-------------------+------------------+------------------+----------+------------------+------------------+------------------+\n",
      "|21ccb8006a50da9b1...|2022-04-01 13:00:00|                no|1.8333333333333333|     136.2|            1959.0|             17.13|             42.25|\n",
      "|52b413067437984fb...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|            1561.0| 5.715000000000001|           17.7175|\n",
      "|a5c7281e5955cd080...|2022-04-01 19:00:00|                no| 2.033333333333333|     136.2|             701.0|             5.715|            18.375|\n",
      "|025c4a64d4348a818...|2022-04-01 19:00:00|                no| 2.033333333333333|     136.2|             288.0|0.8899999999999999|            6.0625|\n",
      "|48c0b5669ed50a0dc...|2022-04-01 13:00:00|                no|1.8333333333333333|     136.2|             170.0|              0.25|              4.25|\n",
      "|57fa54b44d1e25d82...|2022-04-01 16:00:00|                no| 2.033333333333333|     136.2|             739.5|1.8050000000000002|             9.125|\n",
      "|65e6fa122a48ea774...|2022-04-01 08:00:00|                no|0.7133333333333333|     136.2|            1797.5|             9.945|              29.5|\n",
      "|79014a1d9bac0ae75...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|            1560.0|              11.9|             29.25|\n",
      "|97c6416993b8151c4...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|             540.0|               0.9|              6.75|\n",
      "|cf2a68db5901a09fc...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|            3248.0|             14.29|              34.6|\n",
      "|d133de68d7dfb2069...|2022-04-01 07:00:00|                no|               0.6|     136.2|            1932.0|              17.8|             44.75|\n",
      "|6794d3cb4e473ce49...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|1639.3333333333333| 6.713333333333334|             21.25|\n",
      "|dfe93dd8fbdee8c0f...|2022-04-01 13:00:00|                no|1.8333333333333333|     136.2|             269.8|             1.108|               7.5|\n",
      "|eeace255ebbdbaf09...|2022-04-01 18:00:00|                no|2.0666666666666664|     136.2|            1527.0|              5.17|              17.6|\n",
      "|0c16d63294bfa9a1d...|2022-04-01 13:00:00|                no|1.8333333333333333|     136.2|            1154.0|             13.55|              34.0|\n",
      "|15b7f4c0463be79f2...|2022-04-01 18:00:00|                no|2.0666666666666664|     136.2|             915.0|             2.285|            10.375|\n",
      "|3c07027096c12ad3f...|2022-04-01 13:00:00|                no|1.8333333333333333|     136.2|            1390.0|             12.06|31.416666666666668|\n",
      "|5da013ec65199c182...|2022-04-01 17:00:00|                no|2.1233333333333335|     136.2|            2198.0|             14.42|             36.75|\n",
      "|6dcd43a953f0c1bdf...|2022-04-01 16:00:00|                no| 2.033333333333333|     136.2|1347.6666666666667|              5.47|23.463333333333335|\n",
      "|c1ffe6edab518145a...|2022-04-01 16:00:00|                no| 2.033333333333333|     136.2|             935.0|            1.1575|             11.96|\n",
      "+--------------------+-------------------+------------------+------------------+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e18fff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/*\n",
      " Compute a deterministic hash for the `left_table_query_string` that will be used throughout\n",
      " all the logic as the field to GROUP BY the data\n",
      "*/\n",
      "CREATE OR REPLACE TEMPORARY VIEW entity_dataframe AS (\n",
      "    SELECT *,\n",
      "        event_timestamp AS entity_timestamp\n",
      "        \n",
      "            ,CONCAT(\n",
      "                \n",
      "                CAST(event_timestamp AS STRING)\n",
      "            ) AS fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "        \n",
      "            ,CONCAT(\n",
      "                \n",
      "                    CAST(taxi_id AS STRING),\n",
      "                \n",
      "                CAST(event_timestamp AS STRING)\n",
      "            ) AS fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "        \n",
      "    FROM feast_entity_df_9d3dca1686e34da1a44161af016807d8\n",
      ");\n",
      "\n",
      "---EOS---\n",
      "\n",
      "\n",
      "\n",
      "CREATE OR REPLACE TEMPORARY VIEW fv_chi_station_reads_hourly__cleaned AS (\n",
      "\n",
      "    WITH fv_chi_station_reads_hourly__entity_dataframe AS (\n",
      "        SELECT\n",
      "            \n",
      "            entity_timestamp,\n",
      "            fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "        FROM entity_dataframe\n",
      "        GROUP BY\n",
      "            \n",
      "            entity_timestamp,\n",
      "            fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "    ),\n",
      "\n",
      "    /*\n",
      "     This query template performs the point-in-time correctness join for a single feature set table\n",
      "     to the provided entity table.\n",
      "\n",
      "     1. We first join the current feature_view to the entity dataframe that has been passed.\n",
      "     This JOIN has the following logic:\n",
      "        - For each row of the entity dataframe, only keep the rows where the `timestamp_field`\n",
      "        is less than the one provided in the entity dataframe\n",
      "        - If there a TTL for the current feature_view, also keep the rows where the `timestamp_field`\n",
      "        is higher the the one provided minus the TTL\n",
      "        - For each row, Join on the entity key and retrieve the `entity_row_unique_id` that has been\n",
      "        computed previously\n",
      "\n",
      "     The output of this CTE will contain all the necessary information and already filtered out most\n",
      "     of the data that is not relevant.\n",
      "    */\n",
      "\n",
      "    fv_chi_station_reads_hourly__subquery AS (\n",
      "        SELECT\n",
      "            event_timestamp as event_timestamp,\n",
      "            created as created_timestamp,\n",
      "            \n",
      "            \n",
      "                precipitation_type as precipitation_type, \n",
      "            \n",
      "                avg_temp as avg_temp, \n",
      "            \n",
      "                total_rain as total_rain\n",
      "            \n",
      "        FROM `feast_entity_df_d29ccafa933f411f874e63745d8c215a`\n",
      "        WHERE event_timestamp <= '2022-04-15T23:00:00'\n",
      "        \n",
      "    ),\n",
      "\n",
      "    fv_chi_station_reads_hourly__base AS (\n",
      "        SELECT\n",
      "            subquery.*,\n",
      "            entity_dataframe.entity_timestamp,\n",
      "            entity_dataframe.fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "        FROM fv_chi_station_reads_hourly__subquery AS subquery\n",
      "        INNER JOIN fv_chi_station_reads_hourly__entity_dataframe AS entity_dataframe\n",
      "        ON TRUE\n",
      "            AND subquery.event_timestamp <= entity_dataframe.entity_timestamp\n",
      "\n",
      "            \n",
      "\n",
      "            \n",
      "    ),\n",
      "\n",
      "    /*\n",
      "     2. If the `created_timestamp_column` has been set, we need to\n",
      "     deduplicate the data first. This is done by calculating the\n",
      "     `MAX(created_at_timestamp)` for each event_timestamp.\n",
      "     We then join the data on the next CTE\n",
      "    */\n",
      "    \n",
      "    fv_chi_station_reads_hourly__dedup AS (\n",
      "        SELECT\n",
      "            fv_chi_station_reads_hourly__entity_row_unique_id,\n",
      "            event_timestamp,\n",
      "            MAX(created_timestamp) as created_timestamp\n",
      "        FROM fv_chi_station_reads_hourly__base\n",
      "        GROUP BY fv_chi_station_reads_hourly__entity_row_unique_id, event_timestamp\n",
      "    ),\n",
      "    \n",
      "\n",
      "    /*\n",
      "     3. The data has been filtered during the first CTE \"*__base\"\n",
      "     Thus we only need to compute the latest timestamp of each feature.\n",
      "    */\n",
      "    fv_chi_station_reads_hourly__latest AS (\n",
      "        SELECT\n",
      "            event_timestamp,\n",
      "            created_timestamp,\n",
      "            fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "        FROM\n",
      "        (\n",
      "            SELECT *,\n",
      "                ROW_NUMBER() OVER(\n",
      "                    PARTITION BY fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "                    ORDER BY event_timestamp DESC,created_timestamp DESC\n",
      "                ) AS row_number\n",
      "            FROM fv_chi_station_reads_hourly__base\n",
      "            \n",
      "                INNER JOIN fv_chi_station_reads_hourly__dedup\n",
      "                USING (fv_chi_station_reads_hourly__entity_row_unique_id, event_timestamp, created_timestamp)\n",
      "            \n",
      "        )\n",
      "        WHERE row_number = 1\n",
      "    )\n",
      "\n",
      "    /*\n",
      "     4. Once we know the latest value of each feature for a given timestamp,\n",
      "     we can join again the data back to the original \"base\" dataset\n",
      "    */\n",
      "    SELECT base.*\n",
      "    FROM fv_chi_station_reads_hourly__base as base\n",
      "    INNER JOIN fv_chi_station_reads_hourly__latest\n",
      "    USING(\n",
      "        fv_chi_station_reads_hourly__entity_row_unique_id,\n",
      "        event_timestamp\n",
      "        \n",
      "            ,created_timestamp\n",
      "        \n",
      "    )\n",
      ")\n",
      "\n",
      "---EOS---\n",
      "\n",
      "\n",
      "\n",
      "CREATE OR REPLACE TEMPORARY VIEW fv_chi_taxi_trips_hourly__cleaned AS (\n",
      "\n",
      "    WITH fv_chi_taxi_trips_hourly__entity_dataframe AS (\n",
      "        SELECT\n",
      "            taxi_id,\n",
      "            entity_timestamp,\n",
      "            fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "        FROM entity_dataframe\n",
      "        GROUP BY\n",
      "            taxi_id,\n",
      "            entity_timestamp,\n",
      "            fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "    ),\n",
      "\n",
      "    /*\n",
      "     This query template performs the point-in-time correctness join for a single feature set table\n",
      "     to the provided entity table.\n",
      "\n",
      "     1. We first join the current feature_view to the entity dataframe that has been passed.\n",
      "     This JOIN has the following logic:\n",
      "        - For each row of the entity dataframe, only keep the rows where the `timestamp_field`\n",
      "        is less than the one provided in the entity dataframe\n",
      "        - If there a TTL for the current feature_view, also keep the rows where the `timestamp_field`\n",
      "        is higher the the one provided minus the TTL\n",
      "        - For each row, Join on the entity key and retrieve the `entity_row_unique_id` that has been\n",
      "        computed previously\n",
      "\n",
      "     The output of this CTE will contain all the necessary information and already filtered out most\n",
      "     of the data that is not relevant.\n",
      "    */\n",
      "\n",
      "    fv_chi_taxi_trips_hourly__subquery AS (\n",
      "        SELECT\n",
      "            event_timestamp as event_timestamp,\n",
      "            created as created_timestamp,\n",
      "            taxi_id AS taxi_id,\n",
      "            \n",
      "                avg_trip_time as avg_trip_time, \n",
      "            \n",
      "                avg_trip_dist as avg_trip_dist, \n",
      "            \n",
      "                avg_trip_fare as avg_trip_fare\n",
      "            \n",
      "        FROM `feast_entity_df_86b110111c1d4cb9805a1bc1239e460e`\n",
      "        WHERE event_timestamp <= '2022-04-15T23:00:00'\n",
      "        \n",
      "    ),\n",
      "\n",
      "    fv_chi_taxi_trips_hourly__base AS (\n",
      "        SELECT\n",
      "            subquery.*,\n",
      "            entity_dataframe.entity_timestamp,\n",
      "            entity_dataframe.fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "        FROM fv_chi_taxi_trips_hourly__subquery AS subquery\n",
      "        INNER JOIN fv_chi_taxi_trips_hourly__entity_dataframe AS entity_dataframe\n",
      "        ON TRUE\n",
      "            AND subquery.event_timestamp <= entity_dataframe.entity_timestamp\n",
      "\n",
      "            \n",
      "\n",
      "            \n",
      "            AND subquery.taxi_id = entity_dataframe.taxi_id\n",
      "            \n",
      "    ),\n",
      "\n",
      "    /*\n",
      "     2. If the `created_timestamp_column` has been set, we need to\n",
      "     deduplicate the data first. This is done by calculating the\n",
      "     `MAX(created_at_timestamp)` for each event_timestamp.\n",
      "     We then join the data on the next CTE\n",
      "    */\n",
      "    \n",
      "    fv_chi_taxi_trips_hourly__dedup AS (\n",
      "        SELECT\n",
      "            fv_chi_taxi_trips_hourly__entity_row_unique_id,\n",
      "            event_timestamp,\n",
      "            MAX(created_timestamp) as created_timestamp\n",
      "        FROM fv_chi_taxi_trips_hourly__base\n",
      "        GROUP BY fv_chi_taxi_trips_hourly__entity_row_unique_id, event_timestamp\n",
      "    ),\n",
      "    \n",
      "\n",
      "    /*\n",
      "     3. The data has been filtered during the first CTE \"*__base\"\n",
      "     Thus we only need to compute the latest timestamp of each feature.\n",
      "    */\n",
      "    fv_chi_taxi_trips_hourly__latest AS (\n",
      "        SELECT\n",
      "            event_timestamp,\n",
      "            created_timestamp,\n",
      "            fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "        FROM\n",
      "        (\n",
      "            SELECT *,\n",
      "                ROW_NUMBER() OVER(\n",
      "                    PARTITION BY fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "                    ORDER BY event_timestamp DESC,created_timestamp DESC\n",
      "                ) AS row_number\n",
      "            FROM fv_chi_taxi_trips_hourly__base\n",
      "            \n",
      "                INNER JOIN fv_chi_taxi_trips_hourly__dedup\n",
      "                USING (fv_chi_taxi_trips_hourly__entity_row_unique_id, event_timestamp, created_timestamp)\n",
      "            \n",
      "        )\n",
      "        WHERE row_number = 1\n",
      "    )\n",
      "\n",
      "    /*\n",
      "     4. Once we know the latest value of each feature for a given timestamp,\n",
      "     we can join again the data back to the original \"base\" dataset\n",
      "    */\n",
      "    SELECT base.*\n",
      "    FROM fv_chi_taxi_trips_hourly__base as base\n",
      "    INNER JOIN fv_chi_taxi_trips_hourly__latest\n",
      "    USING(\n",
      "        fv_chi_taxi_trips_hourly__entity_row_unique_id,\n",
      "        event_timestamp\n",
      "        \n",
      "            ,created_timestamp\n",
      "        \n",
      "    )\n",
      ")\n",
      "\n",
      "---EOS---\n",
      "\n",
      "\n",
      "\n",
      "/*\n",
      " Joins the outputs of multiple time travel joins to a single table.\n",
      " The entity_dataframe dataset being our source of truth here.\n",
      " */\n",
      "\n",
      "SELECT taxi_id, event_timestamp, precipitation_type, avg_temp, total_rain, avg_trip_time, avg_trip_dist, avg_trip_fare\n",
      "FROM entity_dataframe\n",
      "\n",
      "LEFT JOIN (\n",
      "    SELECT\n",
      "        fv_chi_station_reads_hourly__entity_row_unique_id\n",
      "        \n",
      "            ,precipitation_type\n",
      "        \n",
      "            ,avg_temp\n",
      "        \n",
      "            ,total_rain\n",
      "        \n",
      "    FROM fv_chi_station_reads_hourly__cleaned\n",
      ") USING (fv_chi_station_reads_hourly__entity_row_unique_id)\n",
      "\n",
      "LEFT JOIN (\n",
      "    SELECT\n",
      "        fv_chi_taxi_trips_hourly__entity_row_unique_id\n",
      "        \n",
      "            ,avg_trip_time\n",
      "        \n",
      "            ,avg_trip_dist\n",
      "        \n",
      "            ,avg_trip_fare\n",
      "        \n",
      "    FROM fv_chi_taxi_trips_hourly__cleaned\n",
      ") USING (fv_chi_taxi_trips_hourly__entity_row_unique_id)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hist_features.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d060f5d",
   "metadata": {},
   "source": [
    "# Get feature from online store for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89eec80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 02:01:45 WARN TaskSetManager: Stage 90 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:01:45 WARN TaskSetManager: Stage 91 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:03:36 WARN TaskSetManager: Stage 97 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:04:43 ERROR TaskSchedulerImpl: Lost executor 0 on 10.0.5.7: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/29 02:04:43 WARN TaskSetManager: Lost task 2.0 in stage 103.0 (TID 206) (10.0.5.7 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/29 02:04:50 WARN TaskSetManager: Lost task 0.0 in stage 105.0 (TID 213) (10.0.5.7 executor 1): FetchFailed(null, shuffleId=18, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n",
      "22/06/29 02:04:50 WARN TaskSetManager: Lost task 0.0 in stage 107.0 (TID 214) (10.0.5.7 executor 1): FetchFailed(null, shuffleId=20, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 20 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:228)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n",
      "22/06/29 02:04:50 WARN TaskSetManager: Lost task 0.0 in stage 108.0 (TID 215) (10.0.5.7 executor 1): FetchFailed(null, shuffleId=18, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 18 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n",
      "22/06/29 02:04:50 WARN TaskSetManager: Stage 99 contains a task of very large size (4691 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/29 02:04:52 WARN TaskSetManager: Stage 116 contains a task of very large size (4615 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 140:==============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+------------------+----------+-------------+-------------+-------------+\n",
      "|taxi_id                                                                                                                         |event_timestamp    |precipitation_type|avg_temp          |total_rain|avg_trip_time|avg_trip_dist|avg_trip_fare|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+------------------+----------+-------------+-------------+-------------+\n",
      "|0c16d63294bfa9a1d6452cfaf53bd8479acb2161f88c3f49cfc760ef4964b9b717e7695b79231a7dae2dc16e9aa22d4ad0feaa49138d1b15a7607e2e9c2b3286|2022-04-01 13:00:00|no                |1.8333333333333333|136.2     |1154.0       |13.55        |34.0         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+------------------+----------+-------------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_df.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895cfaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/23 01:43:31 WARN TaskSetManager: Stage 61 contains a task of very large size (4247 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/23 01:46:02 WARN TaskSetManager: Stage 65 contains a task of very large size (4247 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(taxi_id='0c16d63294bfa9a1d6452cfaf53bd8479acb2161f88c3f49cfc760ef4964b9b717e7695b79231a7dae2dc16e9aa22d4ad0feaa49138d1b15a7607e2e9c2b3286', event_timestamp=datetime.datetime(2022, 4, 1, 13, 0), avg_trip_time=1154.0, avg_trip_dist=13.55, avg_trip_fare=34.0, avg_trip_tips=7.7, total_fare_hour=34.0, total_tips_hour=7.7, trips_count=1, created=datetime.date(2022, 4, 1), precipitation_type='no', avg_temp=1.8333333333333333, total_rain=136.2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f6780",
   "metadata": {},
   "source": [
    "## Materialize Features to online store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1956cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views from \u001b[1m\u001b[32m2022-04-01 00:00:00+00:00\u001b[0m to \u001b[1m\u001b[32m2022-05-30 00:00:00+00:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mfv_chi_taxi_trips_hourly\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:64: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling latest features from spark offline store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 2195/2195 [00:02<00:00, 876.48it/s]\n",
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views from \u001b[1m\u001b[32m2022-04-01 00:00:00+00:00\u001b[0m to \u001b[1m\u001b[32m2022-05-30 00:00:00+00:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mfv_chi_station_reads_hourly\u001b[0m:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:64: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling latest features from spark offline store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/29 02:54:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 02:54:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 02:55:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "fs.materialize(\n",
    "    start_date=datetime(2022, 4, 1),\n",
    "    end_date=datetime(2022, 5, 30),\n",
    "    feature_views=[\"fv_chi_taxi_trips_hourly\"]\n",
    ")\n",
    "\n",
    "fs.materialize(\n",
    "    start_date=datetime(2022, 4, 1),\n",
    "    end_date=datetime(2022, 5, 30),\n",
    "    feature_views=[\"fv_chi_station_reads_hourly\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227f3f4",
   "metadata": {},
   "source": [
    "## Retrieve features from online store\n",
    "\n",
    "![Online Store Data Model](docs/img/feast-data-model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edb6cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taxi_id': ['0c16d63294bfa9a1d6452cfaf53bd8479acb2161f88c3f49cfc760ef4964b9b717e7695b79231a7dae2dc16e9aa22d4ad0feaa49138d1b15a7607e2e9c2b3286'],\n",
       " 'read_id': ['2022-04-01 13:00:00'],\n",
       " 'avg_trip_time': [722.0],\n",
       " 'avg_trip_fare': [10.039999961853027],\n",
       " 'avg_trip_dist': [2.2200000286102295],\n",
       " 'precipitation_type': ['no'],\n",
       " 'avg_temp': [21.823333740234375]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_rows = [{\"taxi_id\": \"0c16d63294bfa9a1d6452cfaf53bd8479acb2161f88c3f49cfc760ef4964b9b717e7695b79231a7dae2dc16e9aa22d4ad0feaa49138d1b15a7607e2e9c2b3286\", \"read_id\": \"2022-04-01 13:00:00\"}]\n",
    "\n",
    "fs.get_online_features(entity_rows=entity_rows,\n",
    "                       features=[\n",
    "                           \"fv_chi_taxi_trips_hourly:avg_trip_time\",\n",
    "                           \"fv_chi_taxi_trips_hourly:avg_trip_dist\",\n",
    "                           \"fv_chi_taxi_trips_hourly:avg_trip_fare\",\n",
    "                           \"fv_chi_station_reads_hourly:precipitation_type\",\n",
    "                           \"fv_chi_station_reads_hourly:avg_temp\"\n",
    "                       ]\n",
    "                      ).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2494a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FeatureView(name = fv_chi_taxi_trips_hourly, entities = ['taxi_id'], stream_source = None, batch_source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"chi_taxi_trips_hourly\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"hdfs://namenode:8020/gold/chicago/f_taxi_trips_hourly\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"chi_taxi_trips_hourly\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"hdfs://namenode:8020/gold/chicago/f_taxi_trips_hourly\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, ttl = 0:00:00, schema = [avg_trip_time-Float32, avg_trip_dist-Float32, avg_trip_fare-Float32, avg_trip_tips-Float32, total_tips_hour-Float32, trips_count-Float32], features = [avg_trip_time-Float32, avg_trip_dist-Float32, avg_trip_fare-Float32, avg_trip_tips-Float32, total_tips_hour-Float32, trips_count-Float32], description = , tags = {}, owner = , projection = FeatureViewProjection(name='fv_chi_taxi_trips_hourly', name_alias=None, features=[avg_trip_time-Float32, avg_trip_dist-Float32, avg_trip_fare-Float32, avg_trip_tips-Float32, total_tips_hour-Float32, trips_count-Float32], join_key_map={}), created_timestamp = 2022-05-23 01:15:19.458642, last_updated_timestamp = 2022-06-29 02:08:32.958585, online = True, materialization_intervals = [(datetime.datetime(2022, 4, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2022, 5, 30, 0, 0, tzinfo=<UTC>)), (datetime.datetime(2022, 4, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2022, 5, 30, 0, 0, tzinfo=<UTC>)), (datetime.datetime(2022, 4, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2022, 5, 30, 0, 0, tzinfo=<UTC>)), (datetime.datetime(2022, 4, 1, 0, 0, tzinfo=<UTC>), datetime.datetime(2022, 5, 30, 0, 0, tzinfo=<UTC>))])>,\n",
       " <FeatureView(name = fv_chi_station_reads_hourly, entities = [], stream_source = None, batch_source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"chi_station_reads_hourly_fv\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"abfss://gold@myfeastadls.dfs.core.windows.net/chicago/weather/station_reads_hourly_fv\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, source = {\n",
       "   \"type\": \"BATCH_SPARK\",\n",
       "   \"timestampField\": \"event_timestamp\",\n",
       "   \"createdTimestampColumn\": \"created\",\n",
       "   \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
       "   \"name\": \"chi_station_reads_hourly_fv\",\n",
       "   \"sparkOptions\": {\n",
       "     \"path\": \"abfss://gold@myfeastadls.dfs.core.windows.net/chicago/weather/station_reads_hourly_fv\",\n",
       "     \"fileFormat\": \"parquet\"\n",
       "   }\n",
       " }, ttl = 0:00:00, schema = [precipitation_type-String, avg_temp-Float32, total_rain-Float32], features = [precipitation_type-String, avg_temp-Float32, total_rain-Float32], description = , tags = {}, owner = , projection = FeatureViewProjection(name='fv_chi_station_reads_hourly', name_alias=None, features=[precipitation_type-String, avg_temp-Float32, total_rain-Float32], join_key_map={}), created_timestamp = 2022-06-29 02:31:31.026365, last_updated_timestamp = 2022-06-29 02:31:31.026365, online = True, materialization_intervals = [])>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ebc67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myfeast",
   "language": "python",
   "name": "myfeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
