{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7aaadeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from feast import FeatureStore\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        config(\"spark.executor.cores\", 1).\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-data-lake-store-sdk:2.3.10\").\\\n",
    "        config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\").\\\n",
    "        config(\"spark.hadoop.fs.azure.account.key.myfeastadls.dfs.core.windows.net\", \"U588DqWLAQQz3zoOkSTij94Me6Wfk+XrmS5Lcd0QePAiGl/LsgkFr76se9scT9w/wagZaEQmppcpTmOZi90DfA==\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"]=\"f35cc17d-4ea3-4b5f-9c1e-e6770f7c7603\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"]=\"5baa3265-c1e8-44fb-bb35-c448ae261d4a\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"]=\"Src8Q~7jJtvkbnsWEzJOu4nS5LnqZOpD4Z_5ia0a\"\n",
    "\n",
    "# spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "# spark.conf.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "# spark.conf.set(\"fs.azure.account.key.myfeastadls.dfs.core.windows.net\", os.environ[\"STORAGE_ACCOUNT_KEY\"])\n",
    "\n",
    "hdfs = \"hdfs://namenode:8020\"\n",
    "fs = FeatureStore(\"./fs_online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73738098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_id\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature Discovery\n",
    "\n",
    "fs = FeatureStore(\"./fs_online\")\n",
    "\n",
    "entities = fs.list_entities()\n",
    "for en in entities:\n",
    "    print(f\"{en.name}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "for f in fs.list_feature_views():\n",
    "    print(f)\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03e484ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = spark.read.\\\n",
    "load(\"hdfs://namenode:8020/gold/chicago/f_taxi_trips_hourly\").filter(F.col(\"created\") <= \"2022-04-11\").\\\n",
    "withColumn(\"read_id\", (F.unix_timestamp(F.col(\"event_timestamp\"),\"dd-MM-yyyy HH:00:00\").cast(\"string\"))).\\\n",
    "select(\"read_id\", \"event_timestamp\").\\\n",
    "distinct().\\\n",
    "sort(\"read_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1322f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(read_id='1648771200', event_timestamp='01-04-2022 00:00:00'),\n",
       " Row(read_id='1648774800', event_timestamp='01-04-2022 01:00:00'),\n",
       " Row(read_id='1648778400', event_timestamp='01-04-2022 02:00:00'),\n",
       " Row(read_id='1648782000', event_timestamp='01-04-2022 03:00:00'),\n",
       " Row(read_id='1648785600', event_timestamp='01-04-2022 04:00:00'),\n",
       " Row(read_id='1648789200', event_timestamp='01-04-2022 05:00:00'),\n",
       " Row(read_id='1648792800', event_timestamp='01-04-2022 06:00:00'),\n",
       " Row(read_id='1648796400', event_timestamp='01-04-2022 07:00:00'),\n",
       " Row(read_id='1648800000', event_timestamp='01-04-2022 08:00:00'),\n",
       " Row(read_id='1648803600', event_timestamp='01-04-2022 09:00:00'),\n",
       " Row(read_id='1648807200', event_timestamp='01-04-2022 10:00:00'),\n",
       " Row(read_id='1648810800', event_timestamp='01-04-2022 11:00:00'),\n",
       " Row(read_id='1648814400', event_timestamp='01-04-2022 12:00:00'),\n",
       " Row(read_id='1648818000', event_timestamp='01-04-2022 13:00:00'),\n",
       " Row(read_id='1648821600', event_timestamp='01-04-2022 14:00:00'),\n",
       " Row(read_id='1648825200', event_timestamp='01-04-2022 15:00:00'),\n",
       " Row(read_id='1648828800', event_timestamp='01-04-2022 16:00:00'),\n",
       " Row(read_id='1648832400', event_timestamp='01-04-2022 17:00:00'),\n",
       " Row(read_id='1648836000', event_timestamp='01-04-2022 18:00:00'),\n",
       " Row(read_id='1648839600', event_timestamp='01-04-2022 19:00:00'),\n",
       " Row(read_id='1648843200', event_timestamp='01-04-2022 20:00:00'),\n",
       " Row(read_id='1648846800', event_timestamp='01-04-2022 21:00:00'),\n",
       " Row(read_id='1648850400', event_timestamp='01-04-2022 22:00:00'),\n",
       " Row(read_id='1648854000', event_timestamp='01-04-2022 23:00:00'),\n",
       " Row(read_id='1648857600', event_timestamp='02-04-2022 00:00:00'),\n",
       " Row(read_id='1648861200', event_timestamp='02-04-2022 01:00:00'),\n",
       " Row(read_id='1648864800', event_timestamp='02-04-2022 02:00:00'),\n",
       " Row(read_id='1648868400', event_timestamp='02-04-2022 03:00:00'),\n",
       " Row(read_id='1648872000', event_timestamp='02-04-2022 04:00:00'),\n",
       " Row(read_id='1648875600', event_timestamp='02-04-2022 05:00:00')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d95685b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:75: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:119: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_features = fs.get_historical_features(\n",
    "    entity_df=entity_df.select(\"event_timestamp\").toPandas(),\n",
    "    features=[\n",
    "        \"fv_chi_station_reads_hourly:precipitation_type\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ab630fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|event_timestamp|precipitation_type|\n",
      "+---------------+------------------+\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 33806)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hist_features.to_spark_df().where(F.col(\"precipitation_type\") != None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77c4834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 209:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+\n",
      "|             trip_id|    event_timestamp|total_tips_hour|\n",
      "+--------------------+-------------------+---------------+\n",
      "|da4802fcc8e9eb10e...|2022-01-04 16:00:00|           null|\n",
      "|5bfb9541130de6ef1...|2022-01-04 15:00:00|           null|\n",
      "|1196baed4ac6d505b...|2022-01-04 16:00:00|           null|\n",
      "|791d74df896226a45...|2022-01-04 17:00:00|           null|\n",
      "|9149bad3504bb6966...|2022-01-04 18:00:00|           null|\n",
      "|d1386a65b4826b912...|2022-01-04 17:00:00|           null|\n",
      "|fb1ed566274f6a66d...|2022-01-04 08:00:00|           null|\n",
      "|4864601f96d54f4cf...|2022-01-04 14:00:00|           null|\n",
      "|7e7dce4bc47f1de9d...|2022-01-04 11:00:00|           null|\n",
      "|ea4daf4e61f0f63eb...|2022-01-04 17:00:00|           null|\n",
      "|99b1203df31b26f1c...|2022-01-04 21:00:00|           null|\n",
      "|208880bcdd04e9815...|2022-01-04 15:00:00|           null|\n",
      "|53c01ec83313fee49...|2022-01-04 17:00:00|           null|\n",
      "|9a9656c2c8f2c27af...|2022-01-04 07:00:00|           null|\n",
      "|1b2a60b8aca7f106d...|2022-01-04 15:00:00|           null|\n",
      "|76aeb811c170e849b...|2022-01-04 07:00:00|           null|\n",
      "|84c1b73a53716992b...|2022-01-04 17:00:00|           null|\n",
      "|929637055de3e69bd...|2022-01-04 15:00:00|           null|\n",
      "|3d4d650b0b10ca90a...|2022-01-04 21:00:00|           null|\n",
      "|d120afb91eb205e88...|2022-01-04 19:00:00|           null|\n",
      "+--------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_fdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a111005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/feast/feature_view.py:256: DeprecationWarning: batch_source and stream_source have been deprecated in favor of `source`.The deprecated fields will be removed in Feast 0.23.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/feast/feature_view.py:194: DeprecationWarning: The `features` parameter is being deprecated in favor of the `schema` parameter. Please switch from using `features` to `schema`. This will also requiring switching feature definitions from using `Feature` to `Field`. Feast 0.21 and onwards will not support the `features` parameter.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature Source Definition\n",
    "station_reads_source = SparkSource(\n",
    "    file_format=\"parquet\",\n",
    "    path=f\"abfss://gold@myfeastadls.dfs.core.windows.net/chicago/weather/station_reads_hourly_fv\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created\",\n",
    "    name=\"chi_station_reads_hourly_fv\"\n",
    ")\n",
    "\n",
    "# Feature Definition\n",
    "station_reads_fv = FeatureView(\n",
    "    name=\"fv_chi_station_reads_hourly\",\n",
    "    features=[\n",
    "        Feature(name=\"precipitation_type\", dtype=ValueType.STRING),\n",
    "        Feature(name=\"avg_temp\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"total_rain\", dtype=ValueType.FLOAT)\n",
    "    ],\n",
    "    batch_source=station_reads_source,\n",
    ")\n",
    "\n",
    "# Entity definition => entity == primary key \n",
    "\n",
    "#trip_entity = Entity(name=\"event_timestamp\", value_type=ValueType.STRING)\n",
    "\n",
    "\n",
    "fs.apply([trip_entity, station_reads_fv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4074a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import Field\n",
    "from feast.infra.offline_stores.contrib.spark_offline_store.spark_source import SparkSource\n",
    "from feast import Feature, FeatureView, ValueType\n",
    "from feast import FeatureStore\n",
    "from datetime import timedelta, datetime\n",
    "from feast import Entity\n",
    "\n",
    "# Feature Source Definition\n",
    "trip_stats_source = SparkSource(\n",
    "    file_format=\"parquet\",\n",
    "    path=f\"{hdfs}/gold/chicago/f_taxi_trips_hourly\",\n",
    "    timestamp_field=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created\",\n",
    "    name=\"chi_taxi_trips_hourly\",\n",
    ")\n",
    "\n",
    "# Feature Definition\n",
    "trip_stats_fv = FeatureView(\n",
    "    name=\"fv_chi_taxi_trips_hourly\",\n",
    "    entities=[\"trip_id\"],\n",
    "    schema=[\n",
    "        Field(name=\"avg_trip_time\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"avg_trip_dist\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"avg_trip_fare\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"avg_trip_tips\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"total_tips_hour\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"trips_count\", dtype=ValueType.FLOAT),\n",
    "    ],\n",
    "    batch_source=trip_stats_source,\n",
    ")\n",
    "\n",
    "# Entity definition => entity == primary key \n",
    "\n",
    "trip_entity = Entity(name=\"trip_id\", value_type=ValueType.STRING)\n",
    "\n",
    "fs.apply([trip_entity, trip_stats_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70813d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"spec\": {\n",
      "    \"name\": \"fv_chi_station_reads_hourly\",\n",
      "    \"features\": [\n",
      "      {\n",
      "        \"name\": \"precipitation_type\",\n",
      "        \"valueType\": \"STRING\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"avg_temp\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"total_rain\",\n",
      "        \"valueType\": \"FLOAT\"\n",
      "      }\n",
      "    ],\n",
      "    \"ttl\": \"0s\",\n",
      "    \"batchSource\": {\n",
      "      \"type\": \"BATCH_SPARK\",\n",
      "      \"timestampField\": \"event_timestamp\",\n",
      "      \"createdTimestampColumn\": \"created\",\n",
      "      \"dataSourceClassType\": \"feast.infra.offline_stores.contrib.spark_offline_store.spark_source.SparkSource\",\n",
      "      \"name\": \"chi_station_reads_hourly_fv\",\n",
      "      \"sparkOptions\": {\n",
      "        \"path\": \"abfss://gold@myfeastadls.dfs.core.windows.net/chicago/weather/station_reads_hourly_fv\",\n",
      "        \"fileFormat\": \"parquet\"\n",
      "      }\n",
      "    },\n",
      "    \"online\": true\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"createdTimestamp\": \"2022-05-19T01:57:59.647444Z\",\n",
      "    \"lastUpdatedTimestamp\": \"2022-05-19T01:57:59.647444Z\"\n",
      "  }\n",
      "}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for f in fs.list_feature_views():\n",
    "    print(f)\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da392803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myfeast",
   "language": "python",
   "name": "myfeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
