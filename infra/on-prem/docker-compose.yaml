version: "3.6"
services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    networks:
      cluster:
        ipv4_address: 10.0.5.2
    container_name: namenode
    volumes:
      - namenode:/hadoop/dfs/name
      - $PWD/workspace-dev/data:/opt/workspace
    environment:
      - CLUSTER_NAME=datamaster
    env_file:
      - ./conf/hadoop.env
    ports:
      - 50070:50070
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1g
  
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    networks:
      cluster:
        ipv4_address: 10.0.5.3
    container_name: datanode
    depends_on: 
      - namenode
    volumes:
      - ./data/datanode:/hadoop/dfs/data
      - $PWD/workspace-dev/data:/opt/workspace
    env_file:
      - ./conf/hadoop.env
    ports:
      - 50075:50075
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2g

  jupyterlab:
    image: spark-standalone-jupyterlab:latest
    networks:
      cluster:
        ipv4_address: 10.0.5.4
    container_name: jupyterlab
    environment:
      - AZURE_TENANT_ID=$AZURE_TENANT_ID
      - AZURE_CLIENT_ID=$AZURE_CLIENT_ID
      - AZURE_CLIENT_SECRET=$AZURE_CLIENT_SECRET
      - STORAGE_ACCOUNT_KEY=$STORAGE_ACCOUNT_KEY
    ports: 
      - 8888:8888
      - 4040:4040
    volumes:
      - $PWD/workspace-dev:/opt/workspace
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2g

  workmachine:
    image: spark-standalone-workmachine:latest
    networks:
      cluster:
        ipv4_address: 10.0.5.8
    container_name: workmachine
    volumes:
      - $PWD/ingest_scripts:/opt/workspace
    environment:
      - INGEST_SCRIPT=ingest_taxy.py
      - PARAMS=2022,4
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2g

  spark-master:
    image: spark-standalone-spark-master:latest
    networks:
      cluster:
        ipv4_address: 10.0.5.5
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./conf/hadoop.env
    volumes:
      - $PWD/workspace-dev/data:/opt/workspace
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2g

  spark-worker-1:
    image: spark-standalone-spark-worker:latest
    networks:
      cluster:
        ipv4_address: 10.0.5.6
    container_name: spark-worker-1
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 3g
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    ports:
      - 8082:8081
    depends_on:
      - spark-master
    env_file:
      - ./conf/hadoop.env
    volumes:
      - $PWD/workspace-dev/data:/opt/workspace
  spark-worker-2:
    image: spark-standalone-spark-worker:latest
    networks:
      cluster:
        ipv4_address: 10.0.5.7
    container_name: spark-worker-2
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 3g
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    ports:
      - 8083:8081
    depends_on:
      - spark-master
    env_file:
      - ./conf/hadoop.env
    volumes:
      - $PWD/workspace-dev/data:/opt/workspace

volumes:
  shared-workspace: 
    name: "shared-file-system"
    driver: local
  datanode:
  namenode:

networks:
  cluster:
    driver: bridge
    ipam:
     driver: default
     config:
       - subnet: 10.0.5.0/16
